Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2020-06-04T01:37:49+05:30

====== 4. Gradient Descent in Practice 2 - Learning Rate ======
Created Thursday 04 June 2020

We know two things for sure:
{{./pasted_image002.png?width=600}}
--------------------
1. "Debugging" gradient descent, how to - Look at J(Œ∏) vs (Œ∏) plot. 
	a. It's increasing or fluctuating - Use a smaller value of **learning rate**, maybe we are overshooting.
	{{./pasted_image.png?width=250}}{{./pasted_image001.png}}
	**Note: Number of iterations is not the value on the X axis, it is theta. #of values is what the distance from the origin tells.**
	b. It's decreasing, but too slow - increase the learning rate by some amount, to decrease the time taken to reach minimum.
	c. It's not changing for a long time - plot and look at the value. Maybe we're at the minimum. **Convergence Test.**
	Looking at plots is better than Automatic convergence tests, because its difficult to guess the threshold, Œµ.
	{{./pasted_image003.png?width=250}}
2. How to choose a learning rate Œ±:
	We do some smart guessing here. Start from a point, multiply by your favorite number, like 3, and plot J and iterations, we can also try from two different points(which work üôÉÔ∏è) and finally decide on a value somewhere between them.
{{./pasted_image005.png?width=600}}
